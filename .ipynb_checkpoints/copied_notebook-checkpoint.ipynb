{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from random import choices\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "import keras\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from numpy import array\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pandas.plotting import lag_plot\n",
    "from sklearn import metrics \n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from random import choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for splitting data into train/test set!\n",
    "\n",
    "def train_test_split(test_share, data):\n",
    "    \n",
    "    #Split data into initial train/test\n",
    "    \n",
    "    train_share = 1 - test_share    \n",
    "    train_size = int(len(data) * train_share)\n",
    "    train_set = data[0:train_size]\n",
    "    test_set = data[train_size:len(data)]    \n",
    "    \n",
    "    \n",
    "    return (train_set, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1111\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "train = pd.read_csv('data/train.csv')\n",
    "train = train.query('date > 85').reset_index(drop = True) \n",
    "train = train[train['weight'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.fillna(train.mean(),inplace=True)\n",
    "\n",
    "train['action'] = ((train['resp'].values) > 0).astype(int)\n",
    "\n",
    "\n",
    "features = [c for c in train.columns if \"feature\" in c]\n",
    "\n",
    "f_mean = np.mean(train[features[1:]].values,axis=0)\n",
    "\n",
    "resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n",
    "\n",
    "#Split into train/test!\n",
    "train_set, test_set = train_test_split(test_share = 0.5, data = train)\n",
    "\n",
    "X_train = train_set.loc[:, train_set.columns.str.contains('feature')]\n",
    "#y_train = (train.loc[:, 'action'])\n",
    "\n",
    "y_train = np.stack([(train_set[c] > 0).astype('int') for c in resp_cols]).T\n",
    "\n",
    "X_test = train_set.loc[:, test_set.columns.str.contains('feature')]\n",
    "#y_train = (train.loc[:, 'action'])\n",
    "\n",
    "y_test = np.stack([(test_set[c] > 0).astype('int') for c in resp_cols]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(\n",
    "    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n",
    "):\n",
    "\n",
    "    inp = tf.keras.layers.Input(shape=(num_columns,))\n",
    "    x = tf.keras.layers.BatchNormalization()(inp)\n",
    "    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n",
    "    for i in range(len(hidden_units)):\n",
    "        x = tf.keras.layers.Dense(hidden_units[i])(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(num_labels)(x)\n",
    "    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=out)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n",
    "        metrics=tf.keras.metrics.AUC(name=\"AUC\"),\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1572/1572 [==============================] - 58s 32ms/step - loss: 0.7018 - AUC: 0.5217\n",
      "Epoch 2/20\n",
      "1572/1572 [==============================] - 51s 33ms/step - loss: 0.6902 - AUC: 0.5405\n",
      "Epoch 3/20\n",
      "1572/1572 [==============================] - 52s 33ms/step - loss: 0.6895 - AUC: 0.5452 0s - loss: 0.6895 - AUC: - ETA: 0s - loss: 0.6895 - AUC: 0.5\n",
      "Epoch 4/20\n",
      "1572/1572 [==============================] - 46s 29ms/step - loss: 0.6892 - AUC: 0.5461\n",
      "Epoch 5/20\n",
      "1572/1572 [==============================] - 41s 26ms/step - loss: 0.6887 - AUC: 0.5486\n",
      "Epoch 6/20\n",
      "1572/1572 [==============================] - 35s 23ms/step - loss: 0.6885 - AUC: 0.5493 2s - loss: 0.6885 - AUC: 0.549 - E\n",
      "Epoch 7/20\n",
      "1572/1572 [==============================] - 32s 21ms/step - loss: 0.6879 - AUC: 0.5518\n",
      "Epoch 8/20\n",
      "1572/1572 [==============================] - 33s 21ms/step - loss: 0.6876 - AUC: 0.5533\n",
      "Epoch 9/20\n",
      "1572/1572 [==============================] - 35s 22ms/step - loss: 0.6876 - AUC: 0.5535\n",
      "Epoch 10/20\n",
      "1572/1572 [==============================] - 35s 22ms/step - loss: 0.6875 - AUC: 0.5537\n",
      "Epoch 11/20\n",
      "1572/1572 [==============================] - 35s 23ms/step - loss: 0.6872 - AUC: 0.5541\n",
      "Epoch 12/20\n",
      "1572/1572 [==============================] - 35s 22ms/step - loss: 0.6869 - AUC: 0.5552\n",
      "Epoch 13/20\n",
      "1572/1572 [==============================] - 35s 22ms/step - loss: 0.6865 - AUC: 0.5572 0s - loss: 0.6865 - AUC\n",
      "Epoch 14/20\n",
      "1572/1572 [==============================] - 35s 23ms/step - loss: 0.6865 - AUC: 0.5567\n",
      "Epoch 15/20\n",
      "1572/1572 [==============================] - 35s 22ms/step - loss: 0.6861 - AUC: 0.5590TA: 0s - loss: 0.6861 -\n",
      "Epoch 16/20\n",
      "1572/1572 [==============================] - 33s 21ms/step - loss: 0.6861 - AUC: 0.5587\n",
      "Epoch 17/20\n",
      "1572/1572 [==============================] - 34s 22ms/step - loss: 0.6859 - AUC: 0.5587\n",
      "Epoch 18/20\n",
      "1572/1572 [==============================] - 36s 23ms/step - loss: 0.6855 - AUC: 0.5608\n",
      "Epoch 19/20\n",
      "1572/1572 [==============================] - 35s 23ms/step - loss: 0.6855 - AUC: 0.5603\n",
      "Epoch 20/20\n",
      "1572/1572 [==============================] - 32s 21ms/step - loss: 0.6854 - AUC: 0.5611\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5000\n",
    "hidden_units = [150, 150, 150]\n",
    "dropout_rates = [0.2, 0.2, 0.2, 0.2]\n",
    "label_smoothing = 1e-2\n",
    "learning_rate = 1e-3\n",
    "\n",
    "clf = create_mlp(\n",
    "    len(features), 5, hidden_units, dropout_rates, label_smoothing, learning_rate\n",
    "    )\n",
    "\n",
    "clf.fit(X_train, y_train, epochs=20, batch_size=500)\n",
    "\n",
    "\n",
    "models = []\n",
    "\n",
    "models.append(clf)\n",
    "\n",
    "th = 0.5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [785708, 785707]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-7dbc45d3d17c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_pred_test_a\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtrain_roc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_train_a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mtest_roc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_test_a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m# # accuracy_test = accuracy_score(y_test, y_pred_test_a)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# # accuracy_train = accuracy_score(y_train, y_pred_train_a)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML_python_course_2020\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML_python_course_2020\\lib\\site-packages\\sklearn\\metrics\\_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    396\u001b[0m                                              max_fpr=max_fpr),\n\u001b[0;32m    397\u001b[0m                                      \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 398\u001b[1;33m                                      sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML_python_course_2020\\lib\\site-packages\\sklearn\\metrics\\_base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[1;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0my_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\ML_python_course_2020\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 256\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [785708, 785707]"
     ]
    }
   ],
   "source": [
    "# y_pred_train = clf.predict_proba(X_train)[:,1]\n",
    "# y_pred_test = clf.predict_proba(X_test)[:,1]\n",
    "y_pred_train_a = clf.predict(X_train)\n",
    "y_pred_test_a = clf.predict(X_test)\n",
    "train_roc = roc_auc_score(y_train, y_pred_train_a)\n",
    "test_roc = roc_auc_score(y_test, y_pred_test_a)\n",
    "# # accuracy_test = accuracy_score(y_test, y_pred_test_a)\n",
    "# # accuracy_train = accuracy_score(y_train, y_pred_train_a)\n",
    "# print(\"Accuracy train: %.2f%%\" % (accuracy_train * 100.0))\n",
    "# print(\"Accuracy test: %.2f%%\" % (accuracy_test * 100.0))\n",
    "#print(\"The train roc is: {}, the test roc is: {}\".format(train_roc, test_roc))\n",
    "#plt.figure(figsize=(30,10))\n",
    "#feat_imp = pd.Series(model.feature_importances_).sort_values(ascending=False)\n",
    "#feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train roc is: 0.5751853972018642, the test roc is: 1\n"
     ]
    }
   ],
   "source": [
    "train_roc = roc_auc_score(y_train, y_pred_train_a)\n",
    "print(\"The train roc is: {}, the test roc is: {}\".format(train_roc, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
